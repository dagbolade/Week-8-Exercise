# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xy-n3fyjnZbu_0fI3lr7QDaaN0Z8MrHi
"""

import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFECV
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

X, y = make_classification(n_samples=1000,
                           n_features=50,
                           n_informative=6,
                           n_redundant=10,
                           n_classes=2,
                           random_state=0
                          )
estimator = DecisionTreeClassifier()
selector = RFECV(estimator, step=1, cv=5, scoring='accuracy')
selector.fit(X, y)
selector.support_

print(selector.ranking_)
print("Optimum number of features: %d" % selector.n_features_)

n_scores = len(selector.cv_results_["mean_test_score"])
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Mean test accuracy")
plt.errorbar(
    range(1, n_scores + 1),
    selector.cv_results_["mean_test_score"],
    yerr=selector.cv_results_["std_test_score"],
)
plt.title("Recursive Feature Elimination \nwith correlated features")
plt.show()

X_Selected=X[:,selector.support_]
x_train, x_test, y_train, y_test = train_test_split(X_Selected, y, test_size=0.30, random_state=4)
clf = SVC(kernel="linear")
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
accuracy = accuracy_score(y_test,y_pred)*100
confusion_mat = confusion_matrix(y_test,y_pred)
print("Accuracy for SVM is:",accuracy)
print("Confusion Matrix")
print(confusion_mat)